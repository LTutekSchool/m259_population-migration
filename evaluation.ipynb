{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1\n",
    "In Bezug auf die Zentrale Fragestellung sind die wichtigesten Felder:\n",
    "* year\n",
    "* population scaled\n",
    "* NetMigration scaled\n",
    "\n",
    "Die ersten beiden Felder sind essenziell, da sie wesentliche Einflussfaktoren auf die Netto-Migration darstellen:\n",
    "\n",
    "* **year** repräsentiert den zeitlichen Verlauf und zeigt Trends oder Muster über die Jahre hinweg.\n",
    "* **population** scaled spiegelt die Grösse der Bevölkerung wider, die direkt mit der Netto-Migration zusammenhängt (grössere Länder tendieren zu höheren Wanderungsbewegungen).\n",
    "\n",
    "Das zuletzt genannte Feld, **netMigration** scaled, ist die Zielvariable, die vorhergesagt werden soll. Ohne dieses \"Ziel\" wäre es nicht möglich, die Fragestellung zu beantworten oder eine sinnvolle Analyse durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2\n",
    "Diese Aufgabe habe ich schon beim vergeleichen der Modelle in **3.2** gemacht. Nochmals zum wiederholen habe ich folgende Messmetriken verwendet:\n",
    "* Mean Squared Error (MSE)\n",
    "* R² Score\n",
    "* Score\n",
    "\n",
    "Meine Wahl nochmals begründet:\n",
    "Ich habe mich für den Random Forest Algorithmus entschieden, da er mit einem Mean Squared Error (MSE) von 0.000116, einem R²-Wert von 0.442980 und einem Score von 0.579 die besten Ergebnisse liefert. Diese Metriken zeigen, dass Random Forest die Datenstruktur effektiv modelliert und eine hohe Vorhersagegenauigkeit bietet. Alternativen wie Gradient Boosting oder lineare Regression erreichen zwar respektable Werte, schneiden jedoch schlechter ab. Random Forest ist zudem robust gegenüber Überanpassung und eignet sich gut für nicht-lineare Zusammenhänge. Das Modell wurde auf den Trainingsdaten trainiert und evaluiert, um optimale Ergebnisse zu gewährleisten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[892 298]\n",
      " [842 349]]\n",
      "Sensitivity (Recall): 0.29\n",
      "Specificity (Precision): 0.54\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"./world_pop_mig_186_countries_scaled.csv\")\n",
    "\n",
    "# Prepare the data for the USA\n",
    "usa_data = data\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = usa_data[['year', 'population scaled']]\n",
    "y = usa_data['netMigration scaled']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Set a threshold using the median of y_test\n",
    "threshold = y_test.median()\n",
    "\n",
    "# Convert predictions and actual values into classes based on the threshold\n",
    "y_pred_classes = (y_pred >= threshold).astype(int)\n",
    "y_test_classes = (y_test >= threshold).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes, labels=[0, 1])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity (precision)\n",
    "sensitivity = recall_score(y_test_classes, y_pred_classes)\n",
    "specificity = precision_score(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.2f}\")\n",
    "print(f\"Specificity (Precision): {specificity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.75      0.61      1190\n",
      "           1       0.54      0.29      0.38      1191\n",
      "\n",
      "    accuracy                           0.52      2381\n",
      "   macro avg       0.53      0.52      0.49      2381\n",
      "weighted avg       0.53      0.52      0.49      2381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_classes, y_pred_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich habe diesen Classification Report aus dem Auftrag LA_1670 übernommen, da er die Ergebnisse der Wahrheitsmatrix noch detaillierter beschreibt und zusätzliche Metriken liefert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4\n",
    "\n",
    "### **Hypothese auf die schwache Leistung des Models**\n",
    "Anfangs zeigte das Modell eine schwache Leistung, was vermutlich an der geringen Anzahl an Trainingsdaten (nur USA-Daten mit 63 Zeilen) und den kleinen Unterschieden in den skalierten Werten lag. Nach dem erneuten Training mit allen Daten des Datensatzes verbesserte sich die Modellleistung, insbesondere bei grösseren Differenzen in der Bevölkerung. Dennoch wurde deutlich, dass selbst mit mehr Daten nur bei extrem grossen Datensprüngen in den Eingabewerten signifikante Veränderungen in den Vorhersagen erzwungen werden konnten. Dies weist darauf hin, dass die Datenmenge und Variabilität der Features entscheidend für die Modellleistung sind, aber möglicherweise weitere Optimierungen erforderlich sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
